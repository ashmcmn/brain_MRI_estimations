{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qBOLD Model Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6pX5KsuPljLvibqZrxXmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashmcmn/brain_MRI_estimations/blob/master/qBOLD_Model_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSWurBLYsLo_"
      },
      "source": [
        "# Intro\n",
        "\n",
        "The qBOLD model/method allows for the evaluation of bio-physiological parameters such as oxygen extraction fraction (OEF), deoxygenated blood volume (DBV) and reversible transverse relaxation rate ($R'_2$). This notebook details various implementations of the method in an attempt to maximise the accuracy of these predictions. If you want to read in detail about the theory of qBOLD, I recommend reading the paper \"Quantitative BOLD: Mapping of human cerebral deoxygenated blood volume and oxygen extraction fraction:  Default state\" by He, X. & Yablonskiy, D. A. from 2007."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVe-2CtDxJ_m"
      },
      "source": [
        "import nibabel as nib\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "!gdown --id '1zTZoh71HXmT3EBPkm0ku5ySEcjX3l98l'\n",
        "img = nib.load('slice4_ASE.nii.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznNvWZx5rJH"
      },
      "source": [
        "hct = 0.34; # hct ratio in small vessels\n",
        "dMagSus = 0.264*10**-6; # ppm, sus difference between fully oxy & deoxy rbc's\n",
        "pGyro = 2.675*10**4; # rads/(secs.Gauss) gyromagnetic ratio\n",
        "fStr = 3*10**4; # Gauss, Field strength\n",
        "oCap = 1.34; # mlO2/gHb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MTpnUvI0_b_"
      },
      "source": [
        "# The Data\n",
        "Here's a quick look into the data provided. Typically, when fetching data from a .nii file it is of shape (a, b, c, d) where a and b define the image resolution, c is the number of slices and d is the number time intervals the data was recorded at (post-excitation). The data sample used here is a single slice of resolution 96x96, recorded at 11 different time intervals. These time intervals are the delays from initial excitation at which the signals are measured. The 'tau' values for this fMRI scan ranged from -16ms to 64ms at a step of 8ms, such that the first three values are -16ms, -8ms and 0ms. Rendering the images in chronological order below visualizes the measurements of signals recorded during the ASE pulse sequence. You can see the signal strength decay over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o760iqSW1MEy"
      },
      "source": [
        "data = img.get_fdata().reshape((96, 96, 11))\n",
        "taus = range(-16,65,8)\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(18, 14))\n",
        "axes = axes.ravel()\n",
        "[a.set_axis_off() for a in axes.ravel()]\n",
        "\n",
        "for t in range(data.shape[2]):\n",
        "  im = axes[t].imshow(data[:,:,t], vmax=255)\n",
        "  plt.colorbar(im, ax=axes[t])\n",
        "  axes[t].set_title('Signal measurements\\noffset {}ms from excitation'.format(taus[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9pC3eQUA7d1"
      },
      "source": [
        "If we a single pixel of the image from an area with varying signal strength over time we can visualize the decay. The general idea is to create a system that, when given data like this, can predict the aforementioned parameters, accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Tz9Iy3LeBIaD",
        "outputId": "acc115b8-b00c-4615-d75c-ace7d1df1f44"
      },
      "source": [
        "excerpt = np.log(data[50,50,:])\n",
        "plt.scatter(taus, excerpt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f014d531c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNlSIOdJDJkX"
      },
      "source": [
        "# Linear Evaluation of Parameters Using Linear Regression\n",
        "Taking a look at the visualisation of data above and compare it the brilliant figure below from the 2017 paper by A.J. Stone and N.P. Blockley. We can see captured signals can be characterized by the qBOLD signal. If we use linear regression on the data, knowing that the signal decay is equivalent to $R'_2$, we can take the gradient of the solution to be $R'_2$. DBV can then be calculated using the intercept of the solution, that just leaves OEF. OEF can be calculated from the deoxyhaemoglobin concentration (dHb) and the haemoglobin concentration (Hb). dHb can be calculated from $R'_2$ and DBV assuming knowledge of other constants, Hb can be inferred given the expected proportion of hematocrit and finally then OEF can be calculated using these values. In practice OEF is simplified into a single equation.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mqvTHTVoS-B-jkNDJRyzv3t_lTwj-zw0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw7qA9GMHQ-Z"
      },
      "source": [
        "# @title Manual Linear Regression (Double-Click to Open)\n",
        "class QBOLD_LR():\n",
        "  def __init__(self, hct, dMagSus, pGyro, fStr, oCap, cTime, tConf):\n",
        "    self.hct = hct\n",
        "    self.dMagSus = dMagSus\n",
        "    self.pGyro = pGyro\n",
        "    self.fStr = fStr\n",
        "    self.oCap = oCap\n",
        "    self.cTime = cTime\n",
        "    [tStart, tEnd, tStep] = tConf\n",
        "    self.taus = np.array(range(tStart, tEnd+1, tStep))*10**-3\n",
        "\n",
        "  def _fit_single(self, xdata):\n",
        "    x = np.vstack([self.taus, np.ones_like(taus)])\n",
        "    idx = np.where(self.taus > self.cTime)[0]\n",
        "    x = np.take(x, idx, axis=1)\n",
        "    xdata = np.take(xdata, idx)\n",
        "    return np.dot(np.linalg.pinv(x).T, xdata.reshape(xdata.shape[0],1)).flatten()\n",
        "\n",
        "  def _fit_single_sk(self, xdata):\n",
        "    idx = np.where(self.taus > self.cTime)[0]\n",
        "    x = np.take(self.taus, idx)\n",
        "    w = np.sqrt(np.diag((1. / x).T))\n",
        "    x = np.vstack([x, np.ones(len(x))]).T\n",
        "    xw = np.dot(w, x) \n",
        "    y = np.take(xdata, idx)\n",
        "    yw = np.dot(y.T, w)\n",
        "\n",
        "    m, c = np.linalg.lstsq(xw, yw.T, rcond=None)[0]\n",
        "    return [m, c]\n",
        "\n",
        "  def fit(self, xdata, method='manual'):\n",
        "    self.data = xdata\n",
        "    dim = self.data.shape\n",
        "    self.lr = np.zeros((*dim[:2],2))\n",
        "\n",
        "    for x in range(dim[0]):\n",
        "      for y in range(dim[1]):\n",
        "        if method == 'manual':\n",
        "          self.lr[x,y,:] = self._fit_single(self.data[x,y])\n",
        "        elif method == 'sk':\n",
        "          self.lr[x,y,:] = self._fit_single_sk(self.data[x,y])\n",
        "\n",
        "  def visualise_qbold(self, x, y):\n",
        "    excerpt = self.data[x, y, :]\n",
        "    lr = self.lr[x, y, :]\n",
        "\n",
        "    line_x = np.linspace(self.taus[0], self.taus[-1], 100).reshape((-1,1)) \n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.axvline(self.cTime, color='black', linestyle='--', label='Tc')\n",
        "    ax.plot(line_x, lr[0]*line_x + lr[1], color='r', label='R2P')\n",
        "    ax.scatter(self.taus, excerpt, label='Original Data')\n",
        "    ax.set_xlabel('Displacement from SE, tau /ms')\n",
        "    ax.set_ylabel('ln(signal)')\n",
        "    legend = ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
        "    plt.show()\n",
        "\n",
        "  def _calculate_parameters(self, x, y):\n",
        "    excerpt = self.data[x, y, :]\n",
        "    lr = self.lr[x, y, :]\n",
        "\n",
        "    r2p = -self.lr[x, y, 0]\n",
        "    dbv = self.lr[x, y, 1] - self.data[x, y, np.where(self.taus == 0)].flatten()[0]\n",
        "    return [r2p, dbv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlvlCQeRI6HS",
        "outputId": "961da1a9-1923-4a33-d9e7-367a6be9038c"
      },
      "source": [
        "model = QBOLD_LR(hct, dMagSus, pGyro, fStr, oCap, 0.017, [-16,64,8])\n",
        "model.fit(np.log(data))\n",
        "\n",
        "res = np.zeros((*data.shape[:2], 2))\n",
        "for x in range(res.shape[0]):\n",
        "  for y in range(res.shape[1]):\n",
        "    res[x][y] = model._calculate_parameters(x, y)\n",
        "\n",
        "df = pd.DataFrame(res.reshape(-1, 2), columns=['R2P', 'DBV'])\n",
        "df['DBV'] = df['DBV'] * 100\n",
        "df.dropna(inplace=True)\n",
        "df.clip(0, inplace=True)\n",
        "print('Mean R2P of {:.2f}Hz, Mean DBV of {:.2f}%'.format(*df.mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean R2P of 5.20Hz, Mean DBV of 8.25%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC-tdSkkZlD3",
        "outputId": "111c4593-c4f4-4079-c4d0-5480d6c4f0f4"
      },
      "source": [
        "model = QBOLD_LR(hct, dMagSus, pGyro, fStr, oCap, 0.017, [-16,64,8])\n",
        "model.fit(np.log(data), method='sk')\n",
        "\n",
        "res = np.zeros((*data.shape[:2], 2))\n",
        "for x in range(res.shape[0]):\n",
        "  for y in range(res.shape[1]):\n",
        "    res[x][y] = model._calculate_parameters(x, y)\n",
        "\n",
        "df = pd.DataFrame(res.reshape(-1, 2), columns=['R2P', 'DBV'])\n",
        "df['DBV'] = df['DBV'] * 100\n",
        "df.dropna(inplace=True)\n",
        "df.clip(0, inplace=True)\n",
        "print('Mean R2P of {:.2f}Hz, Mean DBV of {:.2f}%'.format(*df.mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean R2P of 5.10Hz, Mean DBV of 7.73%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BmErv6HdIqN"
      },
      "source": [
        "# Non-Linear Evaluation of Parameters Using Neural Networks\n",
        "As you've probably noticed, the methods used above extrapolate the parameters from a linear regime. The aim in using neural networks is to train a model that is capable of accurately predicting parameters making use of all the data by using non-linear activation functions. The first problem encountered here is the lack of training data. Not only is the data provided limited but it is also untagged, hence supervised learning would be ruled out. To overcome this problem, I will simulate signals by rearranging the equation for the quadratic exponential profile of $R_2'$ decay. The equation is defined differently in the two regimes separated by the characteristic time, these are equations 7 a & b in the aforementioned 2017 Stone & Blockley paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xofnyOpz5kX"
      },
      "source": [
        "#@title Class for predicting R2P and deriving OEF and DBV (double-click)\n",
        "class QBOLD_NN():\n",
        "  def __init__(self, hct, dMagSus, pGyro, fStr, tConf, oefR, dbvR):\n",
        "    self.hct = hct\n",
        "    self.dMagSus = dMagSus\n",
        "    self.pGyro = pGyro\n",
        "    self.fStr = fStr\n",
        "    [tStart, tEnd, tStep] = tConf\n",
        "    self.taus = np.array(range(tStart, tEnd+1, tStep))*10**-3\n",
        "    self.oefR = oefR\n",
        "    self.dbvR = dbvR\n",
        "\n",
        "  def generate(self, n):\n",
        "    n = np.sqrt(n)\n",
        "    c = 3/(4 * np.pi*self.pGyro*self.fStr*self.dMagSus)\n",
        "\n",
        "    if not n.is_integer():\n",
        "      print('Please give a perfect square number')\n",
        "      return\n",
        "\n",
        "    n = int(n)\n",
        "\n",
        "    tc = 0.016\n",
        "\n",
        "    oef = np.linspace(0.20, 0.70, n)\n",
        "    dbv = np.linspace(0.003, 0.15, n)\n",
        "    data = np.zeros((n**2, 15))\n",
        "\n",
        "    i = 0\n",
        "    for x in oef:\n",
        "      for y in dbv:\n",
        "        r2p = (x * self.hct * y)/c\n",
        "        s0 = 1/np.exp(-11.5*0.074)*np.exp(-0.3*(r2p*-0.016)**2/y)\n",
        "        st = []\n",
        "        for t in self.taus:\n",
        "          if t < tc:\n",
        "            st.append(s0*np.exp(-11.5*0.074)*np.exp(-0.3*(r2p*t)**2/y))\n",
        "          else:\n",
        "            st.append(s0*np.exp(-11.5*0.074)*np.exp(y-r2p*t))\n",
        "        data[i] = [x, y, r2p, s0]+st\n",
        "        i += 1\n",
        "\n",
        "    self.data = pd.DataFrame(data, columns=np.hstack((['OEF', 'DBV', 'R2P', 'S0'],self.taus)))\n",
        "\n",
        "  def _split_data(self, split, ynames):\n",
        "    data = self.data.sample(frac=1)\n",
        "\n",
        "    x = data.to_numpy()[:,4:]\n",
        "    y = data[ynames].to_numpy()\n",
        "    idx = int(split*data.shape[0])\n",
        "\n",
        "    self.trainx = x[:idx,:]\n",
        "    self.trainy = y[:idx,:]\n",
        "    self.testx = x[idx:,:]\n",
        "    self.testy = y[idx:,:]\n",
        "\n",
        "  def create_model(self, ynames=['R2P'], split=0.8, activation='relu', o='adam', lr=0.01, lf='mse', snr=100, n=10):\n",
        "    self.model_config = {}\n",
        "    self.model_config['ynames'] = ynames\n",
        "    self.model_config['split'] = split\n",
        "    self.model_config['activation'] = activation\n",
        "    self.model_config['optimizer'] = o\n",
        "    self.model_config['lr'] = lr\n",
        "    self.model_config['lf'] = lf\n",
        "    self.model_config['snr'] = snr\n",
        "    self.model_config['n'] = n\n",
        "\n",
        "    self._split_data(split, ynames)\n",
        " \n",
        "    self.model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.GaussianNoise(1/(np.sqrt(11)*snr)),\n",
        "      tf.keras.layers.Dense(n, activation=activation),\n",
        "      tf.keras.layers.Dense(len(ynames))\n",
        "    ])\n",
        "\n",
        "    if o == 'adam':\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif o == 'sgd':\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    else:\n",
        "      print('Optimizer not recognised.')\n",
        "      return\n",
        "\n",
        "    if lf == 'mse':\n",
        "      loss = tf.keras.losses.MeanSquaredError()\n",
        "    elif lf == 'mae':\n",
        "      loss = tf.keras.losses.MeanAbsoluteError()\n",
        "    else:\n",
        "      print('Loss function not recognised.')\n",
        "      return\n",
        "\n",
        "    self.model.compile(optimizer=optimizer,\n",
        "              loss=loss)\n",
        "    \n",
        "  def train_model(self, log_transform=False, epochs=100, patience=20, batch_size=1, verbose=0):\n",
        "    self.model_config['log_transform'] = log_transform\n",
        "    self.model_config['epochs'] = epochs\n",
        "    self.model_config['patience'] = patience\n",
        "    self.model_config['batch_size'] = batch_size\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=patience)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
        "\n",
        "    trainx = self.trainx\n",
        "    trainy = self.trainy\n",
        "    testx = self.testx\n",
        "    testy = self.testy\n",
        "\n",
        "    if log_transform:\n",
        "      trainx = np.log(trainx)\n",
        "      testx = np.log(testx)\n",
        "\n",
        "    history = self.model.fit(trainx, trainy, epochs=epochs, validation_data=(testx, testy), batch_size=batch_size, callbacks=[es, mc], verbose=verbose)\n",
        "\n",
        "    self.model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "    if verbose == 1:\n",
        "      plt.plot(history.history['loss'], label='loss')\n",
        "      plt.plot(history.history['val_loss'], label='val_loss')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Error [Hz]')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "    return history\n",
        "\n",
        "  def predict_model(self, data):\n",
        "    if self.model_config['log_transform']:\n",
        "      data = np.log(data)\n",
        "\n",
        "    predictions = self.model.predict(data)\n",
        "\n",
        "    ynames = self.model_config['ynames']\n",
        "    out = np.zeros((data.shape[0],3))\n",
        "\n",
        "    try:\n",
        "      out[:,0] = predictions[:,ynames.index('R2P')]\n",
        "    except ValueError:\n",
        "      print('R2P prediction is required')\n",
        "\n",
        "    try:\n",
        "      out[:,1] = predictions[:,ynames.index('DBV')]\n",
        "    except ValueError:\n",
        "      out[:,1] = 1 - data[:,np.where(self.taus==0)[0][0]]\n",
        "\n",
        "    try:\n",
        "      out[:,2] = predictions[:,ynames.index('OEF')]\n",
        "    except ValueError:\n",
        "      c = 3/(4 * np.pi*self.pGyro*self.fStr*self.dMagSus)\n",
        "      out[:,2] = (c*out[:,0])/(self.hct*out[:,1])\n",
        "      \n",
        "    return pd.DataFrame(out, columns=['R2P(P)', 'DBV(P)', 'OEF(P)'])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DRl-x4MV0Ti3",
        "outputId": "6aa2a647-ed83-4cdc-e89c-09c16a440f08"
      },
      "source": [
        "qboldnn = QBOLD_NN(hct, dMagSus, pGyro, fStr, [-16,64,8], [0.20, 0.70], [0.003, 0.15])\n",
        "qboldnn.generate(10000)\n",
        "qboldnn.create_model(ynames=['R2P', 'DBV'], split=0.8, activation='relu', o='adam', lr=1e-3, lf='mse', snr=100, n=20)\n",
        "qboldnn.train_model(log_transform=True, epochs=1000, patience=20, batch_size=4, verbose=1)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 25.6700 - val_loss: 0.6089\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.60895, saving model to best_model.h5\n",
            "Epoch 2/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5415 - val_loss: 0.4235\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.60895 to 0.42354, saving model to best_model.h5\n",
            "Epoch 3/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2823 - val_loss: 0.0368\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.42354 to 0.03675, saving model to best_model.h5\n",
            "Epoch 4/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0238 - val_loss: 0.0148\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03675 to 0.01480, saving model to best_model.h5\n",
            "Epoch 5/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0133 - val_loss: 0.0118\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01480 to 0.01182, saving model to best_model.h5\n",
            "Epoch 6/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0122 - val_loss: 0.0097\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01182 to 0.00969, saving model to best_model.h5\n",
            "Epoch 7/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0112 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00969 to 0.00879, saving model to best_model.h5\n",
            "Epoch 8/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0106 - val_loss: 0.0076\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00879 to 0.00758, saving model to best_model.h5\n",
            "Epoch 9/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0093 - val_loss: 0.0071\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00758 to 0.00707, saving model to best_model.h5\n",
            "Epoch 10/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0085 - val_loss: 0.0090\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00707\n",
            "Epoch 11/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0083 - val_loss: 0.0064\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00707 to 0.00636, saving model to best_model.h5\n",
            "Epoch 12/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0076 - val_loss: 0.0078\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00636\n",
            "Epoch 13/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0068 - val_loss: 0.0050\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00636 to 0.00499, saving model to best_model.h5\n",
            "Epoch 14/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0062 - val_loss: 0.0035\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00499 to 0.00346, saving model to best_model.h5\n",
            "Epoch 15/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0055 - val_loss: 0.0035\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00346\n",
            "Epoch 16/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0055 - val_loss: 0.0031\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00346 to 0.00307, saving model to best_model.h5\n",
            "Epoch 17/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0050 - val_loss: 0.0030\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00307 to 0.00298, saving model to best_model.h5\n",
            "Epoch 18/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0044 - val_loss: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00298 to 0.00191, saving model to best_model.h5\n",
            "Epoch 19/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0043 - val_loss: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00191 to 0.00188, saving model to best_model.h5\n",
            "Epoch 20/1000\n",
            "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0041 - val_loss: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.00188 to 0.00187, saving model to best_model.h5\n",
            "Epoch 21/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0042 - val_loss: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00187\n",
            "Epoch 22/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0040 - val_loss: 0.0011\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00187 to 0.00110, saving model to best_model.h5\n",
            "Epoch 23/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0037 - val_loss: 9.2142e-04\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00110 to 0.00092, saving model to best_model.h5\n",
            "Epoch 24/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0036 - val_loss: 0.0011\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00092\n",
            "Epoch 25/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0036 - val_loss: 7.1046e-04\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00092 to 0.00071, saving model to best_model.h5\n",
            "Epoch 26/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 5.4046e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00071 to 0.00054, saving model to best_model.h5\n",
            "Epoch 27/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0036 - val_loss: 8.7491e-04\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00054\n",
            "Epoch 28/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0035 - val_loss: 6.0414e-04\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00054\n",
            "Epoch 29/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 7.0607e-04\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00054\n",
            "Epoch 30/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0035 - val_loss: 0.0018\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00054\n",
            "Epoch 31/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 8.3317e-04\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00054\n",
            "Epoch 32/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 8.3896e-04\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00054\n",
            "Epoch 33/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0035 - val_loss: 4.6543e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00054 to 0.00047, saving model to best_model.h5\n",
            "Epoch 34/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0032 - val_loss: 4.8252e-04\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00047\n",
            "Epoch 35/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 6.8193e-04\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00047\n",
            "Epoch 36/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 2.6563e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00047 to 0.00027, saving model to best_model.h5\n",
            "Epoch 37/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0035 - val_loss: 2.8652e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00027\n",
            "Epoch 38/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 6.1997e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00027\n",
            "Epoch 39/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 5.8411e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00027\n",
            "Epoch 40/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 1.9518e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00027 to 0.00020, saving model to best_model.h5\n",
            "Epoch 41/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 7.4699e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00020\n",
            "Epoch 42/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0032 - val_loss: 3.6299e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00020\n",
            "Epoch 43/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 2.1393e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00020\n",
            "Epoch 44/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 4.3845e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00020\n",
            "Epoch 45/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0035 - val_loss: 4.1247e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00020\n",
            "Epoch 46/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 3.3047e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00020\n",
            "Epoch 47/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0031 - val_loss: 0.0012\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00020\n",
            "Epoch 48/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0038 - val_loss: 8.5446e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00020\n",
            "Epoch 49/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 0.0036\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00020\n",
            "Epoch 50/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 2.2596e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00020\n",
            "Epoch 51/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00020\n",
            "Epoch 52/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 0.0016\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00020\n",
            "Epoch 53/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 5.7864e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00020\n",
            "Epoch 54/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00020\n",
            "Epoch 55/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0032 - val_loss: 4.7195e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00020\n",
            "Epoch 56/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 4.9232e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00020\n",
            "Epoch 57/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 3.2444e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00020\n",
            "Epoch 58/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0034 - val_loss: 9.4804e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00020\n",
            "Epoch 59/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0033 - val_loss: 3.4187e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00020\n",
            "Epoch 60/1000\n",
            "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0032 - val_loss: 3.7872e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00020\n",
            "Epoch 00060: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f599fc6c908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "ZefIcwMg6xtx",
        "outputId": "2add733d-71ef-4ed2-a36c-c488b1d2e2bb"
      },
      "source": [
        "excerpt = qboldnn.data.sample(frac=1).iloc[:10]\n",
        "x = excerpt.to_numpy()[:,4:]\n",
        "excerpt.reset_index(inplace=True)\n",
        "predictions = qboldnn.predict_model(x)\n",
        "excerpt = pd.concat([excerpt, predictions], axis=1)\n",
        "excerpt[['OEF', 'DBV', 'R2P', 'OEF(P)', 'DBV(P)', 'R2P(P)']].style.format({\n",
        "    'OEF': '{:.0%}'.format,\n",
        "    'DBV': '{:.0%}'.format,\n",
        "    'OEF(P)': '{:.0%}'.format,\n",
        "    'DBV(P)': '{:.0%}'.format,\n",
        "})"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >OEF</th>        <th class=\"col_heading level0 col1\" >DBV</th>        <th class=\"col_heading level0 col2\" >R2P</th>        <th class=\"col_heading level0 col3\" >OEF(P)</th>        <th class=\"col_heading level0 col4\" >DBV(P)</th>        <th class=\"col_heading level0 col5\" >R2P(P)</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col0\" class=\"data row0 col0\" >29%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col1\" class=\"data row0 col1\" >14%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col2\" class=\"data row0 col2\" >12.514672</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col3\" class=\"data row0 col3\" >30%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col4\" class=\"data row0 col4\" >14%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row0_col5\" class=\"data row0 col5\" >12.501037</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col0\" class=\"data row1 col0\" >22%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col1\" class=\"data row1 col1\" >12%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col2\" class=\"data row1 col2\" >7.713364</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col3\" class=\"data row1 col3\" >24%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col4\" class=\"data row1 col4\" >10%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row1_col5\" class=\"data row1 col5\" >7.664855</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col0\" class=\"data row2 col0\" >21%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col1\" class=\"data row2 col1\" >14%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col2\" class=\"data row2 col2\" >8.545505</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col3\" class=\"data row2 col3\" >24%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col4\" class=\"data row2 col4\" >12%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row2_col5\" class=\"data row2 col5\" >8.480083</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col0\" class=\"data row3 col0\" >37%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col1\" class=\"data row3 col1\" >14%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col2\" class=\"data row3 col2\" >15.609427</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col3\" class=\"data row3 col3\" >36%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col4\" class=\"data row3 col4\" >14%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row3_col5\" class=\"data row3 col5\" >15.626276</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col0\" class=\"data row4 col0\" >60%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col1\" class=\"data row4 col1\" >7%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col2\" class=\"data row4 col2\" >11.813326</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col3\" class=\"data row4 col3\" >60%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col4\" class=\"data row4 col4\" >7%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row4_col5\" class=\"data row4 col5\" >11.823581</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col0\" class=\"data row5 col0\" >55%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col1\" class=\"data row5 col1\" >10%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col2\" class=\"data row5 col2\" >16.469118</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col3\" class=\"data row5 col3\" >53%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col4\" class=\"data row5 col4\" >10%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row5_col5\" class=\"data row5 col5\" >16.489170</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col0\" class=\"data row6 col0\" >29%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col1\" class=\"data row6 col1\" >2%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col2\" class=\"data row6 col2\" >1.827328</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col3\" class=\"data row6 col3\" >31%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col4\" class=\"data row6 col4\" >2%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row6_col5\" class=\"data row6 col5\" >1.834797</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col0\" class=\"data row7 col0\" >50%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col1\" class=\"data row7 col1\" >12%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col2\" class=\"data row7 col2\" >18.710166</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col3\" class=\"data row7 col3\" >48%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col4\" class=\"data row7 col4\" >13%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row7_col5\" class=\"data row7 col5\" >18.738428</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col0\" class=\"data row8 col0\" >29%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col1\" class=\"data row8 col1\" >7%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col2\" class=\"data row8 col2\" >5.737332</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col3\" class=\"data row8 col3\" >30%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col4\" class=\"data row8 col4\" >6%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row8_col5\" class=\"data row8 col5\" >5.737080</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col0\" class=\"data row9 col0\" >70%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col1\" class=\"data row9 col1\" >10%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col2\" class=\"data row9 col2\" >21.645828</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col3\" class=\"data row9 col3\" >77%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col4\" class=\"data row9 col4\" >9%</td>\n",
              "                        <td id=\"T_81807dc0_7120_11eb_ac2b_0242ac1c0002row9_col5\" class=\"data row9 col5\" >21.618980</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f5994489c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYytl1qpnxT2",
        "outputId": "41eb7395-f653-40fe-ddca-0647535482b9"
      },
      "source": [
        "excerpt = qboldnn.data\n",
        "x = excerpt.to_numpy()[:,4:]\n",
        "y = excerpt.to_numpy()[:,2]\n",
        "predictions = qboldnn.predict_model(x)\n",
        "excerpt = pd.concat([excerpt, predictions], axis=1)\n",
        "r2p_error = np.abs(np.diff(excerpt[['R2P', 'R2P(P)']].to_numpy())).mean()\n",
        "dbv_error = np.abs(np.diff(excerpt[['DBV', 'DBV(P)']].to_numpy())).mean()\n",
        "oef_error = np.abs(np.diff(excerpt[['OEF', 'OEF(P)']].to_numpy())).mean()\n",
        "print(f'Total MAE:{r2p_error+dbv_error+oef_error:.4f}, R2P:{r2p_error:.4f}, DBV:{dbv_error:.4f}, OEF:{oef_error:.4f}')"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MAE:0.0424, R2P:0.0155, DBV:0.0035, OEF:0.0234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MUffDC9F-kS",
        "outputId": "cc356ab1-fba2-4b61-b946-eca37687325f"
      },
      "source": [
        "r2p_error+dbv_error+oef_error"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0423749397347072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "ZdHsoVsD7mIO",
        "cellView": "form",
        "outputId": "3c34a348-356c-4a59-e6c6-ccc0fc924845"
      },
      "source": [
        "#@title What does data look like for cases with large loss? (Double-click to open)\n",
        "excerpt = qboldnn.data\n",
        "x = excerpt.to_numpy()[:,4:]\n",
        "y = excerpt.to_numpy()[:,2]\n",
        "predictions = qboldnn.predict_model(x)\n",
        "excerpt = pd.concat([excerpt, predictions], axis=1)\n",
        "excerpt[['OEF', 'DBV', 'R2P', 'OEF(P)', 'DBV(P)', 'R2P(P)']]\n",
        "excerpt['Loss'] = np.abs(y-predictions['R2P(P)'])\n",
        "print(excerpt.sort_values(by=['Loss'])[['OEF', 'DBV', 'R2P', 'OEF(P)', 'DBV(P)', 'R2P(P)', 'Loss']])\n",
        "best = excerpt.sort_values(by=['Loss']).to_numpy()[0]\n",
        "worst = excerpt.sort_values(by=['Loss'], ascending=False).to_numpy()[0]\n",
        "\n",
        "plt.scatter(qboldnn.taus, best[4:15], marker='x', c='green')\n",
        "plt.scatter(qboldnn.taus, worst[4:15], c='red')\n",
        "plt.show()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           OEF       DBV        R2P    OEF(P)    DBV(P)     R2P(P)      Loss\n",
            "2269  0.311111  0.105455   9.899137  0.316709  0.103590   9.899135  0.000002\n",
            "1841  0.290909  0.063879   5.606999  0.303472  0.061234   5.607005  0.000006\n",
            "2052  0.301010  0.080212   7.285134  0.310055  0.077872   7.285128  0.000006\n",
            "8663  0.634343  0.096545  18.478758  0.650523  0.094144  18.478765  0.000008\n",
            "1946  0.295960  0.071303   6.367323  0.306752  0.068794   6.367313  0.000010\n",
            "...        ...       ...        ...       ...       ...        ...       ...\n",
            "198   0.205051  0.148515   9.188574  0.237498  0.127119   9.109349  0.079224\n",
            "97    0.200000  0.147030   8.872650  0.234017  0.124429   8.785878  0.086772\n",
            "199   0.205051  0.150000   9.280441  0.238380  0.127790   9.191454  0.088987\n",
            "98    0.200000  0.148515   8.962254  0.234911  0.125079   8.865545  0.096709\n",
            "99    0.200000  0.150000   9.051858  0.235796  0.125730   8.945211  0.106647\n",
            "\n",
            "[10000 rows x 7 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}